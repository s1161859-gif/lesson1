{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y7Zu1vlZdtH"
      },
      "source": [
        "# Building an AI Agent with LangGraph and AWS Bedrock\n",
        "\n",
        "This notebook demonstrates how to build an AI agent from scratch using LangGraph and AWS Bedrock. We'll create an agent that can use tools to answer complex questions by searching the web.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **LangGraph Fundamentals**: Understanding nodes, edges, and state management\n",
        "2. **Agent Architecture**: How agents think, act, and observe\n",
        "3. **Tool Integration**: Connecting external APIs (Tavily Search) to your agent\n",
        "4. **AWS Bedrock Integration**: Using Amazon Nova Lite for AI reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SzSIZSnZdtI"
      },
      "source": [
        "## Step 1: Install Required Packages\n",
        "\n",
        "We need several packages:\n",
        "- `langgraph`: Framework for building agent workflows as graphs\n",
        "- `langchain-aws`: Integration with AWS Bedrock models\n",
        "- `langchain-community`: Community tools including Tavily search\n",
        "- `tavily-python`: Search API client\n",
        "- `boto3`: AWS SDK for Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zJTFrLFfZdtJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q langgraph langchain-aws langchain-community tavily-python boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5oLOhBmZdtJ"
      },
      "source": [
        "## Step 2: Configure AWS Bedrock and API Keys\n",
        "\n",
        "### Important Security Note\n",
        "Store your credentials in Google Colab secrets:\n",
        "1. Click the 🔑 key icon in the left sidebar\n",
        "2. Add these secrets:\n",
        "   - `awsid`: Your AWS Access Key ID\n",
        "   - `awssecret`: Your AWS Secret Access Key\n",
        "   - `tavily`: Your Tavily API key (get free at https://tavily.com)\n",
        "\n",
        "### What is AWS Bedrock?\n",
        "AWS Bedrock provides access to foundation models from various providers including Amazon's own models. We're using Amazon Nova Lite, a fast and cost-effective model optimized for reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ojsi4bP2ZdtK",
        "outputId": "a478aa8b-568e-49e4-d928-61acdf315df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ AWS Bedrock client initialized\n",
            "✓ Tavily API key configured\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import boto3\n",
        "from langchain_aws import ChatBedrock\n",
        "from langchain_core.tools import tool\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure your AWS credentials using Colab secrets\n",
        "AWS_ACCESS_KEY_ID = userdata.get('awsid')  # Set this in Colab secrets\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get('awssecret')  # Set this in Colab secrets\n",
        "AWS_REGION = \"us-east-1\"  # Change if needed\n",
        "\n",
        "# Configure Tavily API key for search\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily')\n",
        "\n",
        "# Initialize Bedrock client\n",
        "bedrock_runtime = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "# Set up the Bedrock model (using Amazon Nova Lite for cost-effectiveness)\n",
        "llm = ChatBedrock(\n",
        "    client=bedrock_runtime,\n",
        "    model_id=\"amazon.nova-lite-v1:0\",\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0,  # Low temperature for consistent tool calling\n",
        "        \"max_tokens\": 4096\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"✓ AWS Bedrock client initialized\")\n",
        "print(\"✓ Tavily API key configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q3tYRcGZdtK"
      },
      "source": [
        "## Step 3: Import Required Libraries\n",
        "\n",
        "Let's break down what each import does:\n",
        "\n",
        "### Core LangGraph Components\n",
        "- `StateGraph`: Creates the graph structure for our agent\n",
        "- `END`: Special node indicating the workflow should terminate\n",
        "\n",
        "### Type Hints and Utilities\n",
        "- `TypedDict`, `Annotated`: For defining structured state\n",
        "- `operator`: For defining how state updates are handled\n",
        "\n",
        "### Message Types\n",
        "- `HumanMessage`: Represents user input\n",
        "- `AIMessage`: Represents model responses\n",
        "- `SystemMessage`: Sets the behavior/personality of the agent\n",
        "- `ToolMessage`: Contains results from tool executions\n",
        "\n",
        "### LangChain Components\n",
        "- `ChatBedrock`: Wrapper for AWS Bedrock chat models (already initialized above)\n",
        "- `TavilySearchResults`: Tool for web searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3PUJQQiKZdtK"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage, BaseMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcyvgYM0ZdtK"
      },
      "source": [
        "## Step 4: Initialize the Search Tool\n",
        "\n",
        "### What is Tavily?\n",
        "Tavily is a search API optimized for AI agents. Unlike traditional search engines, it returns concise, relevant information perfect for LLMs to process.\n",
        "\n",
        "### Why `max_results=2`?\n",
        "We limit results to keep the context window manageable and reduce API costs. Two results typically provide enough information for most queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "juDYCDTPZdtK",
        "outputId": "51e78650-8dc5-423c-df60-57e3e6bff184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool initialized: tavily_search_results_json\n",
            "Tool description: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the search tool\n",
        "tools = [TavilySearchResults(max_results=2)]\n",
        "\n",
        "print(f\"Tool initialized: {tools[0].name}\")\n",
        "print(f\"Tool description: {tools[0].description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNrmYxeiZdtL"
      },
      "source": [
        "## Step 7: Initialize the Agent\n",
        "\n",
        "### Amazon Nova Lite Model\n",
        "- **Speed**: Optimized for fast responses\n",
        "- **Cost**: Cost-effective foundation model\n",
        "- **Capabilities**: Supports tool calling and reasoning\n",
        "- **Temperature**: Set to 0 for consistent, deterministic outputs\n",
        "\n",
        "### System Prompt Design\n",
        "The system prompt is crucial - it defines:\n",
        "- The agent's personality and tone\n",
        "- How it should use tools\n",
        "- When to search vs. when to answer directly\n",
        "- Response formatting preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eSTcsn90ZdtL"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "    \"\"\"State that gets passed between nodes in the graph.\"\"\"\n",
        "    messages: Annotated[list[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eirrTHxBZdtL"
      },
      "source": [
        "## Step 6: Build the Agent Class\n",
        "\n",
        "### Agent Architecture Overview\n",
        "\n",
        "Our agent follows the **ReAct** pattern (Reasoning + Acting):\n",
        "1. **Reason**: The LLM thinks about what to do\n",
        "2. **Act**: It calls a tool if needed\n",
        "3. **Observe**: It sees the tool's result\n",
        "4. **Repeat**: Until it has enough information to answer\n",
        "\n",
        "### The Three Key Methods\n",
        "\n",
        "#### 1. `call_model()`\n",
        "- Gets the current conversation\n",
        "- Adds system instructions\n",
        "- Calls the LLM\n",
        "- Returns the LLM's response (which may include tool calls)\n",
        "\n",
        "#### 2. `take_action()`\n",
        "- Executes any tools the LLM requested\n",
        "- Handles **parallel tool calling** (multiple tools at once)\n",
        "- Returns tool results as messages\n",
        "\n",
        "#### 3. `exists_action()`\n",
        "- Checks if the LLM wants to use tools\n",
        "- Returns True → go to action node\n",
        "- Returns False → we're done, return to user\n",
        "\n",
        "### Understanding the Graph Structure\n",
        "\n",
        "```\n",
        "START → LLM → [Decision]\n",
        "                ├─ Has tool calls? → ACTION → LLM (loop)\n",
        "                └─ No tool calls? → END\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YdkfliH7ZdtL"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, model, tools, system_message: str):\n",
        "        \"\"\"\n",
        "        Initialize the agent.\n",
        "\n",
        "        Args:\n",
        "            model: The LLM to use (AWS Bedrock in our case)\n",
        "            tools: List of tools available to the agent\n",
        "            system_message: Instructions that define the agent's behavior\n",
        "        \"\"\"\n",
        "        self.system_message = system_message\n",
        "\n",
        "        # Create the graph structure\n",
        "        graph = StateGraph(AgentState)\n",
        "\n",
        "        # Add nodes (the boxes in our flowchart)\n",
        "        graph.add_node(\"llm\", self.call_model)\n",
        "        graph.add_node(\"action\", self.take_action)\n",
        "\n",
        "        # Add conditional edge from LLM\n",
        "        # This is the decision point: \"Should I use a tool or respond?\"\n",
        "        graph.add_conditional_edges(\n",
        "            \"llm\",  # Starting from the LLM node\n",
        "            self.exists_action,  # Use this function to decide\n",
        "            {\n",
        "                True: \"action\",  # If True, go to action node\n",
        "                False: END  # If False, we're done\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Add edge from action back to LLM\n",
        "        # After using a tool, the agent needs to think about the results\n",
        "        graph.add_edge(\"action\", \"llm\")\n",
        "\n",
        "        # Set the entry point (where we start)\n",
        "        graph.set_entry_point(\"llm\")\n",
        "\n",
        "        # Compile the graph into a runnable object\n",
        "        self.graph = graph.compile()\n",
        "\n",
        "        # Store tools as a dictionary for easy lookup\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "\n",
        "        # Bind tools to the model\n",
        "        # This tells the model what tools are available\n",
        "        self.model = model.bind_tools(tools)\n",
        "\n",
        "    def call_model(self, state: AgentState):\n",
        "        \"\"\"\n",
        "        Call the LLM with the current conversation history.\n",
        "\n",
        "        This is where the agent \"thinks\" about what to do next.\n",
        "        \"\"\"\n",
        "        messages = state['messages']\n",
        "\n",
        "        # Add system message at the beginning\n",
        "        messages_with_system = [SystemMessage(content=self.system_message)] + messages\n",
        "\n",
        "        # Call the model\n",
        "        print(\"\\n🤔 Agent is thinking...\")\n",
        "        response = self.model.invoke(messages_with_system)\n",
        "\n",
        "        # Return as a state update (will be added to messages list)\n",
        "        return {'messages': [response]}\n",
        "\n",
        "    def take_action(self, state: AgentState):\n",
        "        \"\"\"\n",
        "        Execute the tools that the LLM requested.\n",
        "\n",
        "        Supports parallel tool calling - the model can request multiple\n",
        "        tools at once for efficiency.\n",
        "        \"\"\"\n",
        "        # Get the last message (which contains tool calls)\n",
        "        last_message = state['messages'][-1]\n",
        "        tool_calls = last_message.tool_calls\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Execute each tool call\n",
        "        for tool_call in tool_calls:\n",
        "            tool_name = tool_call['name']\n",
        "            tool_args = tool_call['args']\n",
        "\n",
        "            print(f\"\\n🔧 Calling tool: {tool_name}\")\n",
        "            print(f\"   Arguments: {tool_args}\")\n",
        "\n",
        "            # Find and call the tool\n",
        "            tool = self.tools[tool_name]\n",
        "            result = tool.invoke(tool_args)\n",
        "\n",
        "            print(f\"   Result preview: {str(result)[:100]}...\")\n",
        "\n",
        "            # Create a ToolMessage with the result\n",
        "            results.append(\n",
        "                ToolMessage(\n",
        "                    content=str(result),\n",
        "                    tool_call_id=tool_call['id']\n",
        "                )\n",
        "            )\n",
        "\n",
        "        print(\"\\n↩️  Going back to the model with results...\")\n",
        "        return {'messages': results}\n",
        "\n",
        "    def exists_action(self, state: AgentState):\n",
        "        \"\"\"\n",
        "        Check if the last message contains any tool calls.\n",
        "\n",
        "        This is the decision function for our conditional edge.\n",
        "        \"\"\"\n",
        "        last_message = state['messages'][-1]\n",
        "        return len(last_message.tool_calls) > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mixzg_QZdtM"
      },
      "source": [
        "# The AWS Bedrock model is already initialized above as 'llm'\n",
        "# We'll use it directly in the agent\n",
        "\n",
        "# Define the agent's behavior\n",
        "system_message = \"\"\"You are a helpful AI assistant with access to web search.\n",
        "\n",
        "When answering questions:\n",
        "1. Use search when you need current information (weather, news, recent events)\n",
        "2. You can make multiple searches if needed to fully answer the question\n",
        "3. Synthesize information from search results into clear, concise answers\n",
        "4. If you can answer without searching (general knowledge), do so directly\n",
        "5. Always cite your sources when using search results\n",
        "\n",
        "Be conversational and helpful!\"\"\"\n",
        "\n",
        "# Create the agent using the AWS Bedrock model we initialized earlier\n",
        "agent = Agent(llm, tools, system_message)\n",
        "\n",
        "print(\"✓ Using AWS Bedrock model (Amazon Nova Lite)\")\n",
        "print(\"✓ System message defined\")\n",
        "print(\"✓ Agent initialized and ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSs78WKWZdtM"
      },
      "source": [
        "## Step 8: Visualize the Agent's Graph\n",
        "\n",
        "### Understanding the Visualization\n",
        "- **Rectangles**: Nodes (actions the agent can take)\n",
        "- **Diamonds**: Conditional decisions\n",
        "- **Arrows**: Flow of execution\n",
        "- **Loops**: The agent can cycle through thinking and acting\n",
        "\n",
        "This visual representation helps us understand exactly how our agent will process requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NtPL_h23ZdtM",
        "outputId": "d2543be0-7122-4a3f-aa74-ac08c28d0d30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not generate graph visualization: type object 'Agent' has no attribute 'graph'\n",
            "This is optional - the agent will still work!\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    # Generate and display the graph visualization\n",
        "    display(Image(Agent.graph.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not generate graph visualization: {e}\")\n",
        "    print(\"This is optional - the agent will still work!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L3sHQixZdtM"
      },
      "source": [
        "## Step 9: Test the Agent - Simple Query\n",
        "\n",
        "### What to Expect\n",
        "For weather queries, the agent will:\n",
        "1. Recognize it needs current information\n",
        "2. Call the Tavily search tool\n",
        "3. Process the search results\n",
        "4. Formulate a natural language response\n",
        "\n",
        "Watch the execution flow in the output below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pCoIYanYZdtM",
        "outputId": "49c867bb-79a1-4f67-caf1-13cc3716c946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'system_message' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-491829209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Helper function to run queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Create the agent using the AWS Bedrock model we initialized earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mask_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'system_message' is not defined"
          ]
        }
      ],
      "source": [
        "# Helper function to run queries\n",
        "# Create the agent using the AWS Bedrock model we initialized earlier\n",
        "agent = Agent(llm, tools, system_message)\n",
        "\n",
        "def ask_agent(question: str):\n",
        "    \"\"\"Ask the agent a question and return the response.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create input in the format the agent expects\n",
        "    input_messages = {'messages': [HumanMessage(content=question)]}\n",
        "\n",
        "    # Run the agent\n",
        "    result = agent.graph.invoke(input_messages)\n",
        "\n",
        "    # Extract the final response\n",
        "    final_message = result['messages'][-1]\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Final Answer:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(final_message.content)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test 1: Simple weather query\n",
        "result1 = ask_agent(\"What is the weather in Hong Kong?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08lpR1HKZdtN"
      },
      "source": [
        "## Step 10: Test Parallel Tool Calling\n",
        "\n",
        "### What is Parallel Tool Calling?\n",
        "When the agent needs information about multiple independent things, modern LLMs can request multiple tools **at the same time** rather than sequentially.\n",
        "\n",
        "### Why is This Useful?\n",
        "- **Faster**: No waiting for one search to complete before starting another\n",
        "- **Efficient**: Fewer round trips to the LLM\n",
        "- **Natural**: Mimics how humans gather information\n",
        "\n",
        "Watch how the agent calls search twice **before** going back to think!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xQ4v9juJZdtN",
        "outputId": "f7876965-bbf6-47c0-a853-acf16d19bd1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ask_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2057125150.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test 2: Parallel tool calling (two independent searches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the weather in Hong Kong and Los Angeles?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ask_agent' is not defined"
          ]
        }
      ],
      "source": [
        "# Test 2: Parallel tool calling (two independent searches)\n",
        "result2 = ask_agent(\"What is the weather in Hong Kong and Los Angeles?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XYSX_00ZdtN"
      },
      "source": [
        "## Step 11: Test Multi-Step Reasoning\n",
        "\n",
        "### Sequential vs Parallel Tool Calling\n",
        "\n",
        "This query requires **sequential** reasoning:\n",
        "1. First search: \"Who won the Super Bowl in 2024?\"\n",
        "2. Process the result (Kansas City Chiefs)\n",
        "3. Second search: \"GDP of Missouri\" (where the Chiefs are based)\n",
        "\n",
        "The agent can't do step 3 without the answer from step 1!\n",
        "\n",
        "### Observing the Reasoning Process\n",
        "Notice how the agent:\n",
        "- Makes the first search\n",
        "- Goes **back to the LLM** to think\n",
        "- Then makes the second search\n",
        "- Finally synthesizes both pieces of information\n",
        "\n",
        "This demonstrates true **reasoning** capability, not just tool execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xeSmZW80ZdtN",
        "outputId": "5b1f3e3c-68c8-49e3-daa7-4c6c2c0652e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ask_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1673983092.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test 3: Multi-step reasoning (sequential tool calls)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m result3 = ask_agent(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"Who won the Nobel Prize in Physics 2024? What is the GDP of the state where that person is based?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ask_agent' is not defined"
          ]
        }
      ],
      "source": [
        "# Test 3: Multi-step reasoning (sequential tool calls)\n",
        "result3 = ask_agent(\n",
        "    \"Who won the Nobel Prize in Physics 2024? What is the GDP of the state where that person is based?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7EAhFlXZdtN"
      },
      "source": [
        "## Step 12: Inspect the Full Conversation History\n",
        "\n",
        "### Understanding Message Types\n",
        "\n",
        "The agent state contains all messages in order:\n",
        "- **HumanMessage**: Your questions\n",
        "- **AIMessage**: The LLM's responses (may include tool_calls)\n",
        "- **ToolMessage**: Results returned by tools\n",
        "\n",
        "### Why This Matters\n",
        "This complete history enables:\n",
        "- **Multi-turn conversations**: The agent remembers context\n",
        "- **Debugging**: See exactly what happened at each step\n",
        "- **Learning**: Understand how the agent reasoned\n",
        "- **Persistence**: Could save and resume conversations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WdiOnBOjZdtN",
        "outputId": "9faa725e-b92d-4491-9bc3-9d5b3726ab76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "COMPLETE MESSAGE HISTORY\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'result3' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3354829788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{i}. {msg.__class__.__name__}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'result3' is not defined"
          ]
        }
      ],
      "source": [
        "# Let's examine the full message history from the last query\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPLETE MESSAGE HISTORY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, msg in enumerate(result3['messages'], 1):\n",
        "    print(f\"\\n{i}. {msg.__class__.__name__}:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "        print(\"Tool Calls:\")\n",
        "        for tc in msg.tool_calls:\n",
        "            print(f\"  - {tc['name']}({tc['args']})\")\n",
        "\n",
        "    if msg.content:\n",
        "        content_preview = msg.content[:200] + \"...\" if len(msg.content) > 200 else msg.content\n",
        "        print(f\"Content: {content_preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6qDh83bZdtN"
      },
      "source": [
        "## Step 13: Try Your Own Questions!\n",
        "\n",
        "### Experiment Ideas\n",
        "\n",
        "Try questions that require:\n",
        "1. **No search**: \"What is 2+2?\" or \"Explain photosynthesis\"\n",
        "2. **Single search**: \"What's the current stock price of Apple?\"\n",
        "3. **Parallel searches**: \"Compare the populations of Tokyo and New York\"\n",
        "4. **Sequential reasoning**: \"Who is the CEO of Tesla? What other companies do they run?\"\n",
        "5. **Complex analysis**: \"What are the top 3 news stories today and how are they related?\"\n",
        "\n",
        "### Understanding Limitations\n",
        "The agent can:\n",
        "- ✅ Search for current information\n",
        "- ✅ Reason across multiple searches\n",
        "- ✅ Synthesize information\n",
        "\n",
        "The agent cannot:\n",
        "- ❌ Remember previous conversations (each query is independent)\n",
        "- ❌ Access information not available via search\n",
        "- ❌ Perform actions (only read information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "W59rrhhKZdtO",
        "outputId": "1783275c-9ce3-4d42-d8f4-8a18997caafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ask_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3782939170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What are the latest developments in AI this week?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ask_agent' is not defined"
          ]
        }
      ],
      "source": [
        "# Try your own question!\n",
        "my_question = \"What are the latest developments in AI this week?\"\n",
        "\n",
        "result = ask_agent(my_question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AvkwgvpZdtO"
      },
      "source": [
        "## Key Concepts Summary\n",
        "\n",
        "### 1. **LangGraph State Management**\n",
        "- State flows through nodes\n",
        "- `operator.add` accumulates messages\n",
        "- Complete history enables reasoning\n",
        "\n",
        "### 2. **Agent Architecture (ReAct Pattern)**\n",
        "```\n",
        "Thought → Action → Observation → Thought → ...\n",
        "```\n",
        "- **Thought**: LLM decides what to do\n",
        "- **Action**: Execute tools\n",
        "- **Observation**: Process results\n",
        "- **Loop**: Until task is complete\n",
        "\n",
        "### 3. **Conditional Logic**\n",
        "- Edges can be conditional (decision points)\n",
        "- Enables dynamic workflows\n",
        "- Agent chooses its own path\n",
        "\n",
        "### 4. **Tool Integration**\n",
        "- Tools extend agent capabilities\n",
        "- LLM decides when to use them\n",
        "- Results feed back into reasoning\n",
        "\n",
        "### 5. **Parallel vs Sequential**\n",
        "- **Parallel**: Independent tasks done simultaneously\n",
        "- **Sequential**: Each step depends on previous results\n",
        "- LLM automatically chooses the right strategy\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "To extend this agent, you could:\n",
        "1. **Add more tools**: Calculator, database access, API calls\n",
        "2. **Add memory**: Store conversation history across sessions\n",
        "3. **Add guardrails**: Validate tool inputs/outputs\n",
        "4. **Add human-in-the-loop**: Require approval for certain actions\n",
        "5. **Multi-agent systems**: Have multiple agents collaborate\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
        "- [Tavily Search API](https://tavily.com/)\n",
        "- [LangChain Hub](https://smith.langchain.com/hub)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
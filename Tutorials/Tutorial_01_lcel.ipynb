{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1161859-gif/lesson1/blob/main/Tutorials/Tutorial_01_lcel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rOCnXF8TJtRo",
        "outputId": "16254d1f-31fc-4a41-b8df-5b4fd6d705c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai langchain-community langchain-core langchain-huggingface docarray hnswlib sentence-transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P1k5kOwJtRr"
      },
      "source": [
        "## Setup: Azure OpenAI Model\n",
        "\n",
        "This cell sets up the AzureChatOpenAI model using the provided code with the correct endpoint URL including the ?Hello= parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f891u0J8JtRu",
        "outputId": "358c35d3-2701-40c5-9b4b-ef6495dca326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base URL: https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=/openai/deployments/gpt-4o-mini/\n",
            "API Version: 2024-02-15-preview\n",
            "Deployment: gpt-4o-mini\n",
            "f0Ml4DrKVyeXt19VrdGmmzrqJffWfABGLxTiaeaNkeAZxVX3prznwKxpDoS2H6UXzGLPVdxO\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your Azure OpenAI API key (keep it secret! In Colab, you can use os.environ for security)\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('eduhkkey')\n",
        "\n",
        "# Set up the Azure OpenAI model (using gpt-4o-mini as per docs)\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=\"https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=\",\n",
        "    api_version=\"2024-02-15-preview\",  # Use a recent version\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    temperature=0,  # Low temperature for consistent tool calling\n",
        "    streaming=False,  # Non-streaming for simplicity\n",
        ")\n",
        "\n",
        "# The actual endpoint used internally\n",
        "print(f\"Base URL: {llm.client._client._base_url}\")\n",
        "print(f\"API Version: {llm.openai_api_version}\")\n",
        "print(f\"Deployment: {llm.deployment_name}\")\n",
        "print(os.environ[\"AZURE_OPENAI_API_KEY\"])  # This will print the key—remove in production!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1LWSSQUJtRw"
      },
      "source": [
        "## Section 1: Introduction to LCEL and Runnable Protocol\n",
        "\n",
        "LCEL (LangChain Expression Language) is a declarative way to compose chains of components in LangChain. It uses the Runnable Protocol, which defines standardized methods (invoke, stream, batch) that all components must implement.\n",
        "\n",
        "Key: Components like prompts and models are 'Runnables' that can be chained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vr4-LjDRJtRw",
        "outputId": "a6744f03-ef47-494a-b292-16e28dcc5e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why was the cat sitting on the computer?  \n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Simple prompt template (Runnable)\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Chain with pipe syntax (LCEL)\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke (sync, single)\n",
        "result = chain.invoke({\"topic\": \"cats\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV5SNMfmJtRx"
      },
      "source": [
        "## Section 2: Runnable Interface and Methods\n",
        "\n",
        "Every Runnable supports:\n",
        "- invoke/ainvoke: Sync/async single input.\n",
        "- batch/abatch: Process multiple inputs.\n",
        "- stream/astream: Incremental output.\n",
        "- input_schema/output_schema: Define I/O types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCPlS-8qJtRy",
        "outputId": "684d6b89-833d-41d3-c515-1757062454d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!', \"Why did the dog sit in the shade? \\n\\nBecause he didn't want to become a hot dog!\"]\n",
            "Why do seagulls fly over the ocean?\n",
            "\n",
            "Because if they flew over the bay, they’d be bagels!"
          ]
        }
      ],
      "source": [
        "# Batch example\n",
        "inputs = [{\"topic\": \"cats\"}, {\"topic\": \"dogs\"}]\n",
        "results = chain.batch(inputs)\n",
        "print(results)\n",
        "\n",
        "# Stream example\n",
        "for chunk in chain.stream({\"topic\": \"birds\"}):\n",
        "    print(chunk, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I58-MFSLJtRy"
      },
      "source": [
        "## Section 3: Advanced Features - Fallbacks, Parallelism, Logging\n",
        "\n",
        "Fallbacks: Add backups with .with_fallbacks().\n",
        "Parallelism: Use RunnableParallel/RunnableMap for concurrent steps.\n",
        "Logging: Built-in to LangSmith (setup required)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4JfhpFQvJtRz",
        "outputId": "00361b37-f8d4-4bf4-bc4e-ce6d5d03e98e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'joke': 'Why did the astronaut break up with his girlfriend?  \\n\\nBecause he needed space!', 'fact': 'Why did the Sun go to school?\\n\\nTo get a little brighter!'}\n",
            "Why did the programmer bring a ladder to the fallback test?\n",
            "\n",
            "Because they heard the code had some high expectations, but they needed a backup plan!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# Parallel chain\n",
        "parallel_chain = RunnableParallel(\n",
        "    joke=chain,\n",
        "    fact=prompt | llm | StrOutputParser()  # Reuse\n",
        ")\n",
        "\n",
        "result = parallel_chain.invoke({\"topic\": \"space\"})\n",
        "print(result)\n",
        "\n",
        "# Fallback example (using a secondary model if primary fails)\n",
        "fallback_llm = AzureChatOpenAI(  # Another instance as fallback\n",
        "    azure_endpoint=\"https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=\",\n",
        "    api_version=\"2024-02-15-preview\",\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    temperature=0.5,\n",
        "    max_retries=0  # Immediate fallback\n",
        ")\n",
        "chain_with_fallback = llm.with_fallbacks([fallback_llm])\n",
        "result = (prompt | chain_with_fallback | StrOutputParser()).invoke({\"topic\": \"fallback test\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgoLex0yJtR0"
      },
      "source": [
        "## Section 4: Pipe Syntax vs. RunnableSequence\n",
        "\n",
        "Pipe (`|`) is shorthand for RunnableSequence via operator overloading (__or__). Verbose alternative: Explicitly use RunnableSequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G4RcM6cBJtR0",
        "outputId": "98bc34e2-72de-4dd5-e271-c86751d2b089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Why did the verbose author get kicked out of the party? \\n\\nBecause every time someone asked, \"What\\'s up?\" they wrote an essay instead of just saying, \"Not much!\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 13, 'total_tokens': 49, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CMrahi4dlOb1PwPwJnQxST0sOHB6Y', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--9c72a3e7-c932-454e-9075-281fd28ee30a-0' usage_metadata={'input_tokens': 13, 'output_tokens': 36, 'total_tokens': 49, 'input_token_details': {}, 'output_token_details': {}}\n",
            "content=\"Why did the verbose writer get kicked out of the library?\\n\\nBecause every time someone asked him a question, he'd give a three-hour seminar instead of just one word!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 13, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CMraj76k7Qjzsdrgirax8CU46T8FN', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'low'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--27fc836a-0c73-424e-b227-2f8be364c133-0' usage_metadata={'input_tokens': 13, 'output_tokens': 33, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "# Pipe syntax\n",
        "pipe_chain = prompt | llm\n",
        "\n",
        "# Verbose equivalent\n",
        "sequence_chain = RunnableSequence(prompt, llm)\n",
        "\n",
        "# Both work the same\n",
        "print(pipe_chain.invoke({\"topic\": \"verbose\"}))\n",
        "print(sequence_chain.invoke({\"topic\": \"verbose\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlx15N7-JtR1"
      },
      "source": [
        "## Section 5: Understanding the Pipe Operator (|) and Operator Overloading\n",
        "\n",
        "The `|` symbol in Python is natively the bitwise OR operator (for integers) or set/dict union operator (in Python 3.9+), but libraries like LangChain use a clever (but fully legitimate) technique called **operator overloading** to repurpose it as a \"pipe\" for composing objects, mimicking the Linux/Unix shell pipe (`|`) that chains commands.\n",
        "\n",
        "### How It Works in Python\n",
        "- **Native Behavior**: Without overloading, `a | b` does bitwise OR if `a` and `b` are ints (e.g., `5 | 3` is 7), or unions sets/dicts (e.g., `{\"a\":1} | {\"b\":2}` is `{\"a\":1, \"b\":2}`).\n",
        "- **Overloading Trick**: Python classes can define special methods (dunder methods) to customize operators. For `|`, it's `__or__` (and optionally `__ror__` for reverse). If you implement this in a class, `obj1 | obj2` calls `obj1.__or__(obj2)`, letting you define custom behavior like chaining.\n",
        "- **In LangChain's LCEL**: The `Runnable` class overloads `__or__` to create a `RunnableSequence`. So `prompt | model` returns a new object that pipes the output of `prompt` into `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTiwCb77JtR1"
      },
      "outputs": [],
      "source": [
        "# Let's demonstrate operator overloading with a simple example\n",
        "class SimplePipe:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "\n",
        "    def __or__(self, other):\n",
        "        # This is what happens when you use | operator\n",
        "        def chained(x):\n",
        "            return other.func(self.func(x))\n",
        "        return SimplePipe(chained)\n",
        "\n",
        "    def invoke(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "# Create simple pipe components\n",
        "add_one = SimplePipe(lambda x: x + 1)\n",
        "double = SimplePipe(lambda x: x * 2)\n",
        "\n",
        "# Chain them with | operator (this calls add_one.__or__(double))\n",
        "chain = add_one | double  # Overloads | to chain functions\n",
        "result = chain.invoke(5)  # (5 + 1) * 2 = 12\n",
        "print(f\"Result: {result}\")  # Output: 12\n",
        "\n",
        "# Show what happens under the hood\n",
        "print(f\"Type of chain: {type(chain)}\")\n",
        "print(f\"Chain is a SimplePipe: {isinstance(chain, SimplePipe)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUzJmG8VJtR1"
      },
      "source": [
        "### Why It Feels Like a Pipe\n",
        "\n",
        "- **Inspired by shells**: In Linux, `cmd1 | cmd2` sends output from cmd1 to cmd2 as input. LCEL does the same for data flow (e.g., prompt output → model input).\n",
        "- **Pros**: Makes code concise and intuitive, especially for pipelines.\n",
        "- **Cons**: Can confuse beginners if they're expecting bitwise OR, but context (like importing LangChain) makes it clear.\n",
        "\n",
        "This has been standard in Python for decades and remains unchanged in 2025—it's not going anywhere. Let's see how LangChain implements this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNvau15DJtR2"
      },
      "outputs": [],
      "source": [
        "# Demonstrate native Python operators vs LangChain overloading\n",
        "\n",
        "# Native bitwise OR\n",
        "print(\"Native bitwise OR:\")\n",
        "print(f\"5 | 3 = {5 | 3}\")  # Bitwise OR: 7\n",
        "\n",
        "# Native set union (Python 3.9+)\n",
        "print(\"\\nNative set/dict union:\")\n",
        "set1 = {1, 2, 3}\n",
        "set2 = {3, 4, 5}\n",
        "print(f\"{set1} | {set2} = {set1 | set2}\")\n",
        "\n",
        "dict1 = {\"a\": 1, \"b\": 2}\n",
        "dict2 = {\"b\": 3, \"c\": 4}\n",
        "print(f\"{dict1} | {dict2} = {dict1 | dict2}\")\n",
        "\n",
        "# LangChain overloaded behavior\n",
        "print(\"\\nLangChain overloaded | operator:\")\n",
        "simple_prompt = ChatPromptTemplate.from_template(\"Say hello to {name}\")\n",
        "chained = simple_prompt | llm\n",
        "print(f\"Type of result: {type(chained)}\")\n",
        "print(f\"Result: {chained.invoke({'name': 'Alice'})}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF4suBaUJtR2"
      },
      "source": [
        "### Key Takeaways\n",
        "\n",
        "1. **Not a hack**: Operator overloading is a core Python feature, like NumPy using `+` for array addition.\n",
        "2. **Intuitive design**: `prompt | model | parser` reads left-to-right like Unix pipes.\n",
        "3. **Under the hood**: `prompt | model` calls `prompt.__or__(model)` which returns a `RunnableSequence`.\n",
        "4. **Flexible**: You can implement this pattern in your own classes for domain-specific pipelines.\n",
        "\n",
        "This approach makes LangChain chains both powerful and readable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlqAVhYiJtR2"
      },
      "source": [
        "## Section 6: Data Flow in Chains\n",
        "\n",
        "In chains, output of one component becomes input to the next. E.g., Prompt output → Model input → Parser input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rMEisyVSJtR3",
        "outputId": "53b3c3d2-180c-456a-e518-7343f9776b55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Output: messages=[HumanMessage(content='Tell me a joke about flow', additional_kwargs={}, response_metadata={})]\n",
            "Model Output: content='Why did the river break up with the ocean? \\n\\nBecause it felt like it was always going against the flow!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 13, 'total_tokens': 36, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-CMrcf2W4p5wbU6JFIHUzRzXwkMh7E', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--8120c2bb-6e2a-4057-89b8-46fdf05229f5-0' usage_metadata={'input_tokens': 13, 'output_tokens': 23, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}}\n",
            "Final Output: Why did the river break up with the ocean? \n",
            "\n",
            "Because it felt like it was always going against the flow!\n"
          ]
        }
      ],
      "source": [
        "# Inspect flow\n",
        "prompt_output = prompt.invoke({\"topic\": \"flow\"})\n",
        "print(\"Prompt Output:\", prompt_output)\n",
        "\n",
        "model_output = llm.invoke(prompt_output)\n",
        "print(\"Model Output:\", model_output)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "final_output = parser.invoke(model_output)\n",
        "print(\"Final Output:\", final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aODiXiJqJtR3"
      },
      "source": [
        "## Section 7: Switching Execution Modes\n",
        "\n",
        "Sync → Async: Use ainvoke/astream/abatch.\n",
        "Single → Batch: Pass list to batch/abatch.\n",
        "Non-stream → Streaming: Use stream/astream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZKMmmP0mJtR3",
        "outputId": "1298b4c5-34fe-4637-f1d3-28333629cbb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the developer go broke?\n",
            "\n",
            "Because they kept waiting for their async calls to resolve, but they never got any interest!\n",
            "@@Why@ did@ the@ river@ break@ up@ with@ the@ stream@?\n",
            "\n",
            "@Because@ it@ found@ someone@ deeper@!@@@"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Redefine the LangChain chain (same as Section 1)\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Redefine the LangChain chain to avoid overwrite from Section 5\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Async invoke\n",
        "async def async_invoke():\n",
        "    return await chain.ainvoke({\"topic\": \"async\"})\n",
        "\n",
        "result = await async_invoke()\n",
        "print(result)\n",
        "\n",
        "# Async stream\n",
        "async def async_stream():\n",
        "    async for chunk in chain.astream({\"topic\": \"stream\"}):\n",
        "        print(chunk, end=\"@\")\n",
        "\n",
        "await async_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDtL35pSJtR4"
      },
      "source": [
        "## Section 8: RAG Example with VectorStore and Retriever\n",
        "\n",
        "Build a Retrieval-Augmented Generation chain: Embed docs, retrieve relevant ones, augment prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncUy4TpsJtR4"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.runnables import RunnableMap\n",
        "\n",
        "# Embeddings (use HuggingFace - free, local, no API key required)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"I am a superman\", \"This apple is great\"],\n",
        "    embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "}) | prompt | llm | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"question\": \"Who am I?\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMM_2oXJtR5"
      },
      "source": [
        "## Section 9: Handling Custom Endpoints Without Native Binding\n",
        "\n",
        "If the endpoint lacks function binding, use prompt engineering: Instruct the model to output JSON tool calls, parse, and invoke manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cu99Qb-JtR5"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.tools import render_text_description\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "@tool\n",
        "def weather_search(airport_code: str) -> str:\n",
        "    \"\"\"Search for weather given an airport code.\"\"\"\n",
        "    return f\"Weather for {airport_code}: Sunny, 75°F\"\n",
        "\n",
        "tools = [weather_search]\n",
        "rendered_tools = render_text_description(tools)\n",
        "\n",
        "system_prompt = f\"\"\"You are an assistant with access to tools.\n",
        "{rendered_tools}\n",
        "\n",
        "If relevant, return JSON with 'name' and 'arguments'.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "def invoke_tool(tool_call: dict):\n",
        "    tool_name = tool_call.get(\"name\")\n",
        "    tool_args = tool_call.get(\"arguments\", {})\n",
        "    for t in tools:\n",
        "        if t.name == tool_name:\n",
        "            return t.invoke(tool_args)\n",
        "    raise ValueError(\"Tool not found\")\n",
        "\n",
        "full_chain = chain | RunnableLambda(invoke_tool)\n",
        "result = full_chain.invoke({\"input\": \"What's the weather at SFO?\"})\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}